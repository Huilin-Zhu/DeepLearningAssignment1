{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1 (TF 2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uut_aem5B5q",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/satyaki-mallick/DeepLearningAssignment1/blob/master/Assignment_1.ipynb#scrollTo=7uut_aem5B5q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ORVPadj7rKq"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "<b>Group [fill in group number]</b>\n",
        "* <b> Student 1 </b> : SATYAKI MALLICK + 1410881\n",
        "* <b> Student 2 </b> : HUILIN ZHU+ 1378627\n",
        "\n",
        "**Reading material**\n",
        "* [1] Mikolov, Tomas, et al. \"[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\" Advances in neural information processing systems. 2013. \n",
        "\n",
        "<b><font color='red'>NOTE</font></b> When submitting your notebook, please make sure that the training history of your model is visible in the output. This means that you should **NOT** clean your output cells of the notebook. Make sure that your notebook runs without errors in linear order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6rQwLyiMFu_1"
      },
      "source": [
        "# Question 1 - Keras implementation (10 pt)\n",
        "\n",
        "### Word embeddings\n",
        "Build word embeddings with a Keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training. Use a window size of 2 to train the embeddings (`window_size` in the jupyter notebook). \n",
        "\n",
        "1. Build word embeddings of length 50, 150 and 300 using the Skipgram model\n",
        "2. Build word embeddings of length 50, 150 and 300 using CBOW model\n",
        "3. Analyze the different word embeddings:\n",
        "    - Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in the paper. Do not use existing libraries for this task such as Gensim. \n",
        "Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. $e_{x}$ denotes the embedding of word $x$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.\n",
        "    - Give at least 5 different  examples of analogies.\n",
        "    - Compare the performance on the analogy tasks between the word embeddings and briefly discuss your results.\n",
        "\n",
        "4. Discuss:\n",
        "  - Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n",
        "\n",
        "\n",
        "<b>HINT</b> See practical 3.1 for some helpful code to start this assignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctoyAoX1AI6T"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x5VOelR7BYQ1",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vCnATRPgBZEd",
        "outputId": "45e49492-73cc-4a50-8f1b-3cacb0753367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# other helpful libraries\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors as nn\n",
        "from matplotlib import pylab\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qBNCPtOoBbB1",
        "outputId": "b8f1d64c-8c14-4bc3-86d3-6c4c3519a489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(tf.__version__) #  check what version of TF is imported"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WCd0zUO1AKjY"
      },
      "source": [
        "### Import file\n",
        "\n",
        "If you use Google Colab, you need to mount your Google Drive to the notebook when you want to use files that are located in your Google Drive. Paste the authorization code, from the new tab page that opens automatically when running the cell, in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DdjNeehKBd-a",
        "outputId": "ff0157d8-1195-4b7a-a17a-57f88905f607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UjIVBt8YGUaO"
      },
      "source": [
        "Navigate to the folder in which `alice.txt` is located. Make sure to start path with '/content/drive/My Drive/' if you want to load the file from your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iS0-uINUBfic",
        "outputId": "b144b76e-d90e-4350-fc13-0047b197790b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DEvNncnXEGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b40cf2c1-e69d-4b39-db21-151f2e312fe6"
      },
      "source": [
        "cd '/content/drive/My Drive/Colab Notebooks/DeepLearning/Practical3'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepLearning/Practical3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gz8Z5gCDBhSl",
        "colab": {}
      },
      "source": [
        "file_name = 'alice.txt'\n",
        "corpus = open(file_name).readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zkbk32wHANnD"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "See Practical 3.1 for an explanation of the preprocessing steps done below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "37PyOHq2BkY4",
        "colab": {}
      },
      "source": [
        "# Removes sentences with fewer than 3 words\n",
        "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
        "\n",
        "# remove punctuation in text and fit tokenizer on entire corpus\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# convert text to sequence of integer values\n",
        "corpus = tokenizer.texts_to_sequences(corpus)\n",
        "n_samples = sum(len(s) for s in corpus) # total number of words in the corpus\n",
        "V = len(tokenizer.word_index) + 1 # total number of unique words in the corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ILdA_IimBlte",
        "outputId": "8ba7c09e-da20-4dd3-b75d-4f6eb33baffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "n_samples, V"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27165, 2557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fRbpue0WBms6",
        "outputId": "a760d08b-3e60-406d-9c19-41bc7194aae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# example of how word to integer mapping looks like in the tokenizer\n",
        "print(list((tokenizer.word_index.items()))[:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 1), ('and', 2), ('to', 3), ('a', 4), ('it', 5)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Er86VxH9BqI9",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "window_size = 2\n",
        "window_size_corpus = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc_S3hY2EamG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def save_embeddings_old(model, model_name):\n",
        "#   weights = model.get_weights()\n",
        "#   embedding = weights[0]\n",
        "#   f = open(model_name + '.txt', 'w')\n",
        "#   f.write(\" \".join([str(V-1), str(dim)]))\n",
        "#   f.write(\"\\n\")\n",
        "\n",
        "#   for word,i in tokenizer.word_index.items():\n",
        "#     f.write(word)\n",
        "#     f.write(\" \")\n",
        "#     f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
        "#     f.write(\"\\n\")\n",
        "#   f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVUwWvbBBzJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_embeddings(model, model_name):\n",
        "  weights = model.get_weights()\n",
        "  embedding = weights[0]\n",
        "  np.savetxt('new_' + model_name + '.txt',embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M0sU1JORATvX"
      },
      "source": [
        "## Task 1.1 - Skipgram\n",
        "Build word embeddings of length 50, 150 and 300 using the Skipgram model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Udp1xKcDBu0v",
        "colab": {}
      },
      "source": [
        "#prepare data for skipgram\n",
        "def generate_data_skipgram(corpus, window_size, V):\n",
        "    # TODO Implement here\n",
        "    # HINT: see Practical 3.1\n",
        "    maxlen = window_size*2\n",
        "    all_in = []\n",
        "    all_out = []\n",
        "    for words in corpus:\n",
        "        L = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            p = index - window_size\n",
        "            n = index + window_size + 1\n",
        "                    \n",
        "            in_words = []\n",
        "            labels = []\n",
        "            for i in range(p, n):\n",
        "                if i != index and 0 <= i < L:\n",
        "                    # Add the input word\n",
        "                    #in_words.append(word)\n",
        "                    all_in.append(word)\n",
        "                    # Add one-hot of the context words\n",
        "                    all_out.append(to_categorical(words[i], V))\n",
        "                                      \n",
        "    return (np.array(all_in),np.array(all_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qXBGNrKTB0tO",
        "colab": {}
      },
      "source": [
        "# create training data\n",
        "x_skipgram , y_skipgram = generate_data_skipgram(corpus,window_size,V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My8ZZJNbsCCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x each word represented as a number and then a window created"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifUAcKq1smqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample of y:\n",
        "# rows represent each word.\n",
        "# columns represent total unique words\n",
        "# sample array for y for input [1, 0, 3, 4, 5, 0, 2, 1]\n",
        "# array([[ 0.,  1.,  0.,  0.,  0.,  0.],\n",
        "#        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
        "#        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
        "#        [ 0.,  0.,  0.,  0.,  1.,  0.],\n",
        "#        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
        "#        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
        "#        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
        "#        [ 0.,  1.,  0.,  0.,  0.,  0.]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhOEHUygm7y3",
        "colab_type": "code",
        "outputId": "0a885351-12ae-42ef-b889-eece649a3162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_skipgram.shape, y_skipgram.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((94556,), (94556, 2557))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UieP5fGDVps5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V = y_skipgram.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J1FTkStoDLFQ",
        "colab": {}
      },
      "source": [
        "def skipgram_architechture(dim):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=V, output_dim=dim, input_length=1, embeddings_initializer='glorot_uniform'))\n",
        "  # not sure about the input length\n",
        "  #model.add(Reshape((94556,), input_shape=(1,94556)))\n",
        "  # above line or below line\n",
        "  model.add(Reshape((dim,)))\n",
        "  model.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "  model.compile(optimizer='adadelta',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHcXjkGZFkm9",
        "colab_type": "text"
      },
      "source": [
        "### Skipgram for Embedding Vector Length 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJX9ht7X3se7",
        "colab_type": "code",
        "outputId": "0c7996be-08b4-4ceb-9993-b1ab36febbfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "dim = 50\n",
        "skipgram50 = skipgram_architechture(dim)\n",
        "skipgram50.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1, 50)             127850    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2557)              130407    \n",
            "=================================================================\n",
            "Total params: 258,257\n",
            "Trainable params: 258,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yxb3ulOlD1Z2"
      },
      "source": [
        "<b>HINT</b>: To increase training speed of your model, you can use the free available GPU power in Google Colab. Go to `Edit` --> `Notebook Settings` --> select `GPU` under `hardware accelerator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a9eg0xPoDP9B",
        "outputId": "759263af-6c9e-427c-bf54-da3a04c4d035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# train skipgram model\n",
        "skipgram50.fit(x_skipgram, y_skipgram, batch_size=64, epochs=10)\n",
        "save_embeddings(skipgram50, 'skipgram_50')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 7.8463 - accuracy: 4.0188e-04\n",
            "Epoch 2/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8455 - accuracy: 9.4124e-04\n",
            "Epoch 3/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8447 - accuracy: 0.0019\n",
            "Epoch 4/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8438 - accuracy: 0.0045\n",
            "Epoch 5/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8430 - accuracy: 0.0085\n",
            "Epoch 6/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8422 - accuracy: 0.0134\n",
            "Epoch 7/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8413 - accuracy: 0.0202\n",
            "Epoch 8/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8405 - accuracy: 0.0259\n",
            "Epoch 9/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8396 - accuracy: 0.0341\n",
            "Epoch 10/10\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 7.8388 - accuracy: 0.0418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yip30sMdF89H",
        "colab_type": "text"
      },
      "source": [
        "### Skipgram for embedding vector length 150"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eGEHlkQ4DShO",
        "outputId": "674bcc25-6e22-497a-f73a-ce268fde81d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "# save embeddings for vectors of length 50, 150 and 300 using skipgram model\n",
        "dim = 150\n",
        "skipgram150 = skipgram_architechture(dim)\n",
        "skipgram150.summary()\n",
        "skipgram150.fit(x_skipgram, y_skipgram, batch_size=64, epochs=10)\n",
        "save_embeddings(skipgram150, 'skipgram_150')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 1, 150)            383550    \n",
            "_________________________________________________________________\n",
            "reshape_5 (Reshape)          (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2557)              386107    \n",
            "=================================================================\n",
            "Total params: 769,657\n",
            "Trainable params: 769,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1478/1478 [==============================] - 10s 7ms/step - loss: 7.8462 - accuracy: 3.7015e-04\n",
            "Epoch 2/10\n",
            "1478/1478 [==============================] - 10s 7ms/step - loss: 7.8453 - accuracy: 5.1821e-04\n",
            "Epoch 3/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8445 - accuracy: 7.6145e-04\n",
            "Epoch 4/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8436 - accuracy: 0.0011\n",
            "Epoch 5/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8428 - accuracy: 0.0019\n",
            "Epoch 6/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8419 - accuracy: 0.0040\n",
            "Epoch 7/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8410 - accuracy: 0.0062\n",
            "Epoch 8/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8402 - accuracy: 0.0088\n",
            "Epoch 9/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8393 - accuracy: 0.0159\n",
            "Epoch 10/10\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 7.8384 - accuracy: 0.0212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zxqAAvIFzNI",
        "colab_type": "text"
      },
      "source": [
        "### Skipgram for embedding vector length 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0ahwbVAFOUU",
        "colab_type": "code",
        "outputId": "5323865e-a281-4efe-868b-c1d8ca8479e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "dim = 300\n",
        "skipgram300 = skipgram_architechture(dim)\n",
        "skipgram300.fit(x_skipgram, y_skipgram, batch_size=32, epochs=10)\n",
        "save_embeddings(skipgram300, 'skipgram_300')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8459 - accuracy: 3.9130e-04\n",
            "Epoch 2/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8444 - accuracy: 6.6627e-04\n",
            "Epoch 3/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8429 - accuracy: 0.0014\n",
            "Epoch 4/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8415 - accuracy: 0.0022\n",
            "Epoch 5/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8400 - accuracy: 0.0084\n",
            "Epoch 6/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8385 - accuracy: 0.0127\n",
            "Epoch 7/10\n",
            "2955/2955 [==============================] - 21s 7ms/step - loss: 7.8371 - accuracy: 0.0164\n",
            "Epoch 8/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8356 - accuracy: 0.0223\n",
            "Epoch 9/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8341 - accuracy: 0.0257\n",
            "Epoch 10/10\n",
            "2955/2955 [==============================] - 20s 7ms/step - loss: 7.8327 - accuracy: 0.0301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b4z9Lt6pAZEw"
      },
      "source": [
        "## Task 1.2 - CBOW\n",
        "\n",
        "Build word embeddings of length 50, 150 and 300 using CBOW model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KMhI_sWFTXDW",
        "colab": {}
      },
      "source": [
        "# prepare data for CBOW\n",
        "\n",
        "# create training data\n",
        "\n",
        "# create CBOW architecture\n",
        "\n",
        "# train CBOW model\n",
        "\n",
        "# save embeddings for vectors of length 50, 150 and 300 using CBOW model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZMJwlS7dETh",
        "colab_type": "text"
      },
      "source": [
        "Prepare data for CBOW\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZmhahwTdR_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data_CBOW(corpus, window_size, V):\n",
        "    maxlen = window_size*2\n",
        "    all_in = []\n",
        "    all_out = [] #one real label wt\n",
        "    for line in corpus:\n",
        "        sentence_length = len(line)\n",
        "      \n",
        "        for index, word in enumerate(line):  #for each word in the line, we create a little neighborhood [left,right]\n",
        "            left = index - window_size\n",
        "            right = index + window_size + 1\n",
        "      \n",
        "            in_words = []   #neighbor words of wt, used as input to predict wt       \n",
        "      \n",
        "            for i in range(left, right):\n",
        "                if 0 <= i < sentence_length and i != index:\n",
        "                    # Add the input word\n",
        "                    in_words.append(line[i])\n",
        "\n",
        "            \n",
        "            all_in.append(in_words)\n",
        "            all_out.append(to_categorical(word,V))\n",
        "      \n",
        "    all_in = sequence.pad_sequences(all_in, maxlen=maxlen)\n",
        "                                      \n",
        "    return (np.array(all_in),np.array(all_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQqNXVdPddWg",
        "colab_type": "text"
      },
      "source": [
        "Create training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28BDAMUxdh_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x , y = generate_data_CBOW(corpus,window_size,V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iswy4EU3gr12",
        "colab_type": "text"
      },
      "source": [
        "Create CBOW architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du7xuYdIgw6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cbow_architechture(dim):\n",
        "    cbow = Sequential()\n",
        "    cbow.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=window_size*2))\n",
        "    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
        "    cbow.add(Dense(V, kernel_initializer='glorot_uniform', activation='softmax'))\n",
        "    #multiclass classification->categorical_crossentropy loss, optimizer->PPT02 p24\n",
        "    cbow.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "    return cbow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNfaATp3g1Y0",
        "colab_type": "text"
      },
      "source": [
        "###CBOW for Embedding Vector Length 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqtHGSsTG45-",
        "colab_type": "text"
      },
      "source": [
        "Train CBOW model - embedding vector length 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqGNiCVyhy65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimension of word embedding\n",
        "dim = 50\n",
        "\n",
        "cbow = cbow_architechture(dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyZhcahTiKV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(cbow, show_shapes = True, show_layer_names=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFiw8wiAiUo8",
        "colab_type": "code",
        "outputId": "79a65a38-4466-4237-90b2-27ad78da988f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "cbow.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 4, 50)             127850    \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2557)              130407    \n",
            "=================================================================\n",
            "Total params: 258,257\n",
            "Trainable params: 258,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qoQ_z3XiW2G",
        "colab_type": "code",
        "outputId": "827cc952-483b-402b-e39d-648b6bffe87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# train skipgram model\n",
        "cbow.fit(x, y, batch_size=64, epochs=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 7.8466 - accuracy: 5.8899e-04\n",
            "Epoch 2/10\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 7.8463 - accuracy: 7.3624e-04\n",
            "Epoch 3/10\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 7.8461 - accuracy: 0.0010\n",
            "Epoch 4/10\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 7.8458 - accuracy: 0.0014\n",
            "Epoch 5/10\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 7.8456 - accuracy: 0.0019\n",
            "Epoch 6/10\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 7.8454 - accuracy: 0.0025\n",
            "Epoch 7/10\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 7.8451 - accuracy: 0.0039\n",
            "Epoch 8/10\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 7.8449 - accuracy: 0.0053\n",
            "Epoch 9/10\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 7.8446 - accuracy: 0.0077\n",
            "Epoch 10/10\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 7.8444 - accuracy: 0.0097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f327d0fdc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n_XLpoSizLy",
        "colab_type": "text"
      },
      "source": [
        "Save embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFa9ypDURn1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_embeddings(cbow, 'cbow_{dim}'.format(dim=dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hIlAEr9G_sb",
        "colab_type": "text"
      },
      "source": [
        "###CBOW for Embedding Vector Length 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xhQJ-pOgjrp4"
      },
      "source": [
        "Train CBOW model - embedding vector length 150"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVb7BxTdjoyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimension of word embedding\n",
        "dim = 150\n",
        "\n",
        "cbow = cbow_architechture(dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FginlQRMj4aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(cbow, show_shapes = True, show_layer_names=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi7D2JxNj7qX",
        "colab_type": "code",
        "outputId": "0ed9a763-8651-4a7e-d5ec-165efcfcd799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "cbow.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 4, 150)            383550    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2557)              386107    \n",
            "=================================================================\n",
            "Total params: 769,657\n",
            "Trainable params: 769,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19djsLqij_dq",
        "colab_type": "code",
        "outputId": "5709e7d6-0eb0-4d02-81f6-6986a4252576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# train skipgram model\n",
        "cbow.fit(x, y, batch_size=64, epochs=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8466 - accuracy: 4.4174e-04\n",
            "Epoch 2/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8463 - accuracy: 4.7856e-04\n",
            "Epoch 3/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8460 - accuracy: 5.5218e-04\n",
            "Epoch 4/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8458 - accuracy: 8.0987e-04\n",
            "Epoch 5/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8455 - accuracy: 9.9393e-04\n",
            "Epoch 6/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8453 - accuracy: 0.0015\n",
            "Epoch 7/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8450 - accuracy: 0.0020\n",
            "Epoch 8/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8448 - accuracy: 0.0024\n",
            "Epoch 9/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8445 - accuracy: 0.0034\n",
            "Epoch 10/10\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 7.8443 - accuracy: 0.0045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f327d2a2a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snC7VXXHj2nO",
        "colab_type": "text"
      },
      "source": [
        "Save embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGpTElB1kHh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_embeddings(cbow, 'cbow_{dim}'.format(dim=dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiVIWzmDHG9t",
        "colab_type": "text"
      },
      "source": [
        "###CBOW for Embedding Vector Length 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vVnwNWvpkPNd"
      },
      "source": [
        "Train CBOW model - embedding vector length 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJZJ7oGkRNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimension of word embedding\n",
        "dim = 300\n",
        "\n",
        "cbow = cbow_architechture(dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UXwCXe9oM1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(cbow, show_shapes = True, show_layer_names=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5uo15a5oR9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbow.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVRh42vmoUmP",
        "colab_type": "code",
        "outputId": "98fcec3d-0c1c-4cf8-e866-d8c065c7c5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# train skipgram model\n",
        "cbow.fit(x, y, batch_size=64, epochs=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8462 - accuracy: 4.4174e-04\n",
            "Epoch 2/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8459 - accuracy: 5.5218e-04\n",
            "Epoch 3/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8456 - accuracy: 8.4668e-04\n",
            "Epoch 4/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8453 - accuracy: 0.0012\n",
            "Epoch 5/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8451 - accuracy: 0.0015\n",
            "Epoch 6/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8448 - accuracy: 0.0018\n",
            "Epoch 7/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8445 - accuracy: 0.0024\n",
            "Epoch 8/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8442 - accuracy: 0.0034\n",
            "Epoch 9/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8440 - accuracy: 0.0048\n",
            "Epoch 10/10\n",
            "425/425 [==============================] - 4s 9ms/step - loss: 7.8437 - accuracy: 0.0059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f327d1ac438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuCa4_ILoX1K",
        "colab_type": "text"
      },
      "source": [
        "Save embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGM8z6adGoE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_embeddings(cbow, 'cbow_{dim}'.format(dim=dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rr2BAI4nAfnG"
      },
      "source": [
        "## Task 1.3 - Analogy function\n",
        "\n",
        "Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in [1]. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. \n",
        "\n",
        "In a perfect scenario, we would like that this analogy ( $e_{king} - e_{queen} + e_{woman}$) results in the embedding of the word \"man\". However, it does not always result in exactly the same word embedding. The result of the formula is called the expected or the predicted word embedding. In this context, \"man\" is called the true or the actual word $t$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.  \n",
        "\n",
        "You have to answer an analogy function using each embedding for both CBOW and Skipgram model. This means that for each analogy we have 6 outputs. Show the true word (with distance similarity value between predicted embedding and true word embedding, i.e. `sim1`) , the predicted word (with distance similarity value between predicted embedding and the embedding of the word in the vocabulary that is closest to this predicted embedding, i.e. `sim2`) and a boolean answer whether the predicted word **exactly** equals the true word. \n",
        "\n",
        "<b>HINT</b>: to visualize the results of the analogy tasks , you can print them in a table. An example is given below.\n",
        "\n",
        "\n",
        "| Analogy task | True word (sim1)  | Predicted word (sim2) | Embedding | Correct?|\n",
        "|------|------|------|------|------|\n",
        "|  queen is to king as woman is to ?\t | man (sim1) | predictd_word(sim2) | SG_50 | True / False|\n",
        "\n",
        "* Give at least 5 different  examples of analogies.\n",
        "* Compare the performance on the analogy s between the word embeddings and briefly discuss your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rALd-jzJgs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed(word, embedding, vocab_size=V, tokenizer=tokenizer):\n",
        "  int_word = tokenizer.texts_to_sequences([word])[0]\n",
        "  bin_word = to_categorical(int_word, V)\n",
        "  return np.dot(bin_word, embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAdMZSW7tJfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def closest_word(predicted_embedding, word1_embedding, word2_embedding, word3_embedding, embedding):\n",
        "  exclude_words = [predicted_embedding, word1_embedding, word2_embedding, word3_embedding]\n",
        "  exclude_index = []\n",
        "  for i in range(V):\n",
        "    for j in exclude_words:\n",
        "      if np.array_equal(j, embedding[[i]]):\n",
        "        exclude_index.append(i)\n",
        "\n",
        "  include_index = [x for x in range(V) if x not in exclude_index]\n",
        "\n",
        "  biggest_dist = 0\n",
        "  index = 0\n",
        "  for i in include_index:  \n",
        "    each_word = embedding[[i]]\n",
        "    dist = cosine_similarity(predicted_embedding, each_word)   #sim1\n",
        "    if dist > biggest_dist:\n",
        "      biggest_dist = dist\n",
        "      index = i\n",
        "  \n",
        "  mylist = list((tokenizer.word_index.items()))\n",
        "\n",
        "  closest_word = [item[0] for item in mylist if item[1] == index]\n",
        "  # print(least_dist)\n",
        "  # print(least_dist[0][0])\n",
        "  # print(closest_word)\n",
        "  return biggest_dist[0][0], closest_word[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3F4TwToLPjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analogy(word1, word2, word3, true_word):\n",
        "\n",
        "  analogy = word2 + ' is to ' + word1 + ' as ' + word3 + ' is to?'\n",
        "  df = pd.DataFrame(columns=['Analogy task', 'True word(sim1)', 'Predicted word(sim2)', 'Embedding', 'Correct?'])\n",
        "  models = ['new_skipgram_50', 'new_skipgram_150', 'new_skipgram_300', 'new_cbow_50', 'new_cbow_150', 'new_cbow_300']\n",
        "\n",
        "  dummy = []\n",
        "  for i, model in enumerate(models):\n",
        "    embedding = np.loadtxt(model + '.txt')\n",
        "    \n",
        "    word1_embedding = embed(word1, embedding)\n",
        "    word2_embedding = embed(word2, embedding)\n",
        "    word3_embedding = embed(word3, embedding)\n",
        "    predicted_embedding = word1_embedding - word2_embedding + word3_embedding\n",
        "\n",
        "    sim1 = cosine_similarity(predicted_embedding, embed(true_word, embedding))   #sim1 with true word\n",
        "\n",
        "    pair = closest_word(predicted_embedding, word1_embedding, word2_embedding, word3_embedding, embedding)\n",
        "    \n",
        "    predicted_word = pair[1]\n",
        "    sim2 = pair[0]\n",
        "    \n",
        "    \n",
        "    sim1 = sim1[0][0]\n",
        "    sim1 = '({val})'.format(val=sim1)\n",
        "    t_word = true_word + sim1\n",
        "    \n",
        "    \n",
        "    sim2 = '({val})'.format(val=sim2)\n",
        "    p_word = predicted_word + sim2\n",
        "\n",
        "    df.loc[i] = [analogy] + [t_word] + [p_word] + [model] + [true_word == predicted_word]\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uoXEYoFwgtK",
        "colab_type": "code",
        "outputId": "3cce1ebb-a43c-42e3-b73d-2985867cd350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "analogy('king', 'queen', 'woman', 'man')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>queen is to king as woman is to?</td>\n",
              "      <td>man(0.17645969789489707)</td>\n",
              "      <td>another(0.44990029377209617)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>queen is to king as woman is to?</td>\n",
              "      <td>man(0.19671153018823417)</td>\n",
              "      <td>smallest(0.2675138931382219)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>queen is to king as woman is to?</td>\n",
              "      <td>man(0.07420692745887787)</td>\n",
              "      <td>helpless(0.22614571125328742)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>queen is to king as woman is to?</td>\n",
              "      <td>man(-0.1834220056211924)</td>\n",
              "      <td>untwist(0.4833274125243154)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>queen is to king as woman is to?</td>\n",
              "      <td>man(0.04072989896274954)</td>\n",
              "      <td>chair(0.2942734997372393)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>queen is to king as woman is to?</td>\n",
              "      <td>man(-0.047171602193436496)</td>\n",
              "      <td>slate(0.19357305524093144)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Analogy task  ... Correct?\n",
              "0  queen is to king as woman is to?  ...    False\n",
              "1  queen is to king as woman is to?  ...    False\n",
              "2  queen is to king as woman is to?  ...    False\n",
              "3  queen is to king as woman is to?  ...    False\n",
              "4  queen is to king as woman is to?  ...    False\n",
              "5  queen is to king as woman is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qh1BrvlTYhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "808c2aa1-2648-476c-9d4a-08b0dbcd2369"
      },
      "source": [
        "analogy('queen', 'king', 'man', 'woman')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>king is to queen as man is to?</td>\n",
              "      <td>woman(-0.12540904719785675)</td>\n",
              "      <td>blow(0.4647726281299349)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>king is to queen as man is to?</td>\n",
              "      <td>woman(0.08479572510230286)</td>\n",
              "      <td>reeds(0.26357401801243774)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>king is to queen as man is to?</td>\n",
              "      <td>woman(-0.005560951152718523)</td>\n",
              "      <td>glad(0.18838150416320984)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>king is to queen as man is to?</td>\n",
              "      <td>woman(0.048848609622548486)</td>\n",
              "      <td>both(0.45667987666832793)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>king is to queen as man is to?</td>\n",
              "      <td>woman(0.02953608888181832)</td>\n",
              "      <td>dodo(0.25267461233221644)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>king is to queen as man is to?</td>\n",
              "      <td>woman(-0.05101961699696715)</td>\n",
              "      <td>killing(0.19488828603668693)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Analogy task  ... Correct?\n",
              "0  king is to queen as man is to?  ...    False\n",
              "1  king is to queen as man is to?  ...    False\n",
              "2  king is to queen as man is to?  ...    False\n",
              "3  king is to queen as man is to?  ...    False\n",
              "4  king is to queen as man is to?  ...    False\n",
              "5  king is to queen as man is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w6b0OMgV7FL",
        "colab_type": "code",
        "outputId": "88ee8189-d187-4f0c-afd3-fbf9213881e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# french to France as english to England\n",
        "analogy('France', 'french', 'english', 'England')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>french is to France as english is to?</td>\n",
              "      <td>England(0.010037594101873819)</td>\n",
              "      <td>pleasing(0.44191868532603623)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>french is to France as english is to?</td>\n",
              "      <td>England(-0.18921131989457513)</td>\n",
              "      <td>into(0.30822511843626627)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>french is to France as english is to?</td>\n",
              "      <td>England(-0.01815044798698159)</td>\n",
              "      <td>neat(0.1763568314689753)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>french is to France as english is to?</td>\n",
              "      <td>England(0.24215522699192957)</td>\n",
              "      <td>dinah(0.46289635122688705)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>french is to France as english is to?</td>\n",
              "      <td>England(0.14641683462362312)</td>\n",
              "      <td>dunce(0.26379237704003605)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>french is to France as english is to?</td>\n",
              "      <td>England(0.024537948566496975)</td>\n",
              "      <td>handsome(0.18467368743144072)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Analogy task  ... Correct?\n",
              "0  french is to France as english is to?  ...    False\n",
              "1  french is to France as english is to?  ...    False\n",
              "2  french is to France as english is to?  ...    False\n",
              "3  french is to France as english is to?  ...    False\n",
              "4  french is to France as english is to?  ...    False\n",
              "5  french is to France as english is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unC4xOwyWdvf",
        "colab_type": "code",
        "outputId": "085d3535-b1ff-4e9d-9644-35df36035284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# forget to forgetting as remember to remembering\n",
        "analogy('forgetting', 'forget', 'remember', 'remembering')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>forget is to forgetting as remember is to?</td>\n",
              "      <td>remembering(-0.24206488594168962)</td>\n",
              "      <td>mind(0.45148866712301483)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>forget is to forgetting as remember is to?</td>\n",
              "      <td>remembering(0.1439939235583136)</td>\n",
              "      <td>lived(0.30396290069689363)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>forget is to forgetting as remember is to?</td>\n",
              "      <td>remembering(0.08139538661542113)</td>\n",
              "      <td>tray(0.2223919550464544)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>forget is to forgetting as remember is to?</td>\n",
              "      <td>remembering(-0.22620641332522362)</td>\n",
              "      <td>emphasis(0.4888385678577706)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>forget is to forgetting as remember is to?</td>\n",
              "      <td>remembering(-0.07978948882359788)</td>\n",
              "      <td>floor(0.277579552107549)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>forget is to forgetting as remember is to?</td>\n",
              "      <td>remembering(0.02857875796476763)</td>\n",
              "      <td>spoke(0.21356521797240333)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Analogy task  ... Correct?\n",
              "0  forget is to forgetting as remember is to?  ...    False\n",
              "1  forget is to forgetting as remember is to?  ...    False\n",
              "2  forget is to forgetting as remember is to?  ...    False\n",
              "3  forget is to forgetting as remember is to?  ...    False\n",
              "4  forget is to forgetting as remember is to?  ...    False\n",
              "5  forget is to forgetting as remember is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-czespTWuMX",
        "colab_type": "code",
        "outputId": "9733c337-2351-4c36-ac70-83c6c926e0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# hand is to leg as to eye is to ear\n",
        "analogy('knee', 'hand', 'eye', 'ear')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hand is to knee as eye is to?</td>\n",
              "      <td>ear(0.10204160267428564)</td>\n",
              "      <td>crawling(0.4519353000022144)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hand is to knee as eye is to?</td>\n",
              "      <td>ear(-0.04415578613275498)</td>\n",
              "      <td>chuckled(0.28050926165789153)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hand is to knee as eye is to?</td>\n",
              "      <td>ear(0.06165104262429445)</td>\n",
              "      <td>us(0.20435088668743157)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hand is to knee as eye is to?</td>\n",
              "      <td>ear(0.12768119631137134)</td>\n",
              "      <td>very(0.543729961961767)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hand is to knee as eye is to?</td>\n",
              "      <td>ear(0.01635093705827108)</td>\n",
              "      <td>even(0.25069047840317404)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hand is to knee as eye is to?</td>\n",
              "      <td>ear(-0.025087382341028818)</td>\n",
              "      <td>sighed(0.17720160555120615)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Analogy task  ... Correct?\n",
              "0  hand is to knee as eye is to?  ...    False\n",
              "1  hand is to knee as eye is to?  ...    False\n",
              "2  hand is to knee as eye is to?  ...    False\n",
              "3  hand is to knee as eye is to?  ...    False\n",
              "4  hand is to knee as eye is to?  ...    False\n",
              "5  hand is to knee as eye is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8h4tA--XFPy",
        "colab_type": "code",
        "outputId": "d504ca6e-07e7-4945-8f95-37ca370c282e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# he is to his as to she is to hers\n",
        "analogy('his', 'he', 'she', 'hers')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>he is to his as she is to?</td>\n",
              "      <td>hers(-0.07428201388995209)</td>\n",
              "      <td>settle(0.4295097532817805)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>he is to his as she is to?</td>\n",
              "      <td>hers(-0.10372003520778603)</td>\n",
              "      <td>appeared(0.2843083464182061)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>he is to his as she is to?</td>\n",
              "      <td>hers(0.014964269771963981)</td>\n",
              "      <td>shrill(0.19254202025984168)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>he is to his as she is to?</td>\n",
              "      <td>hers(0.01937456269204181)</td>\n",
              "      <td>undoing(0.48574646530538657)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>he is to his as she is to?</td>\n",
              "      <td>hers(0.00012462515723521902)</td>\n",
              "      <td>each(0.24443508736304217)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>he is to his as she is to?</td>\n",
              "      <td>hers(-0.08031098742553902)</td>\n",
              "      <td>arithmetic(0.17426783895637374)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Analogy task  ... Correct?\n",
              "0  he is to his as she is to?  ...    False\n",
              "1  he is to his as she is to?  ...    False\n",
              "2  he is to his as she is to?  ...    False\n",
              "3  he is to his as she is to?  ...    False\n",
              "4  he is to his as she is to?  ...    False\n",
              "5  he is to his as she is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q7ErHY7UB2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "06ee2f51-2412-44a1-bd31-b1856fa74119"
      },
      "source": [
        "analogy('he', 'his', 'hers', 'she')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Analogy task</th>\n",
              "      <th>True word(sim1)</th>\n",
              "      <th>Predicted word(sim2)</th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Correct?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>his is to he as hers is to?</td>\n",
              "      <td>she(-0.061049304855692044)</td>\n",
              "      <td>small(0.4135019658150866)</td>\n",
              "      <td>new_skipgram_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>his is to he as hers is to?</td>\n",
              "      <td>she(-0.04069604577476947)</td>\n",
              "      <td>excellent(0.30856708261080656)</td>\n",
              "      <td>new_skipgram_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>his is to he as hers is to?</td>\n",
              "      <td>she(0.06013155981663805)</td>\n",
              "      <td>cucumber(0.19922338675414952)</td>\n",
              "      <td>new_skipgram_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>his is to he as hers is to?</td>\n",
              "      <td>she(0.07197485680091115)</td>\n",
              "      <td>since(0.4787940904787158)</td>\n",
              "      <td>new_cbow_50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>his is to he as hers is to?</td>\n",
              "      <td>she(-0.050996869568261126)</td>\n",
              "      <td>it(0.24370894178848868)</td>\n",
              "      <td>new_cbow_150</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>his is to he as hers is to?</td>\n",
              "      <td>she(0.07266589781509537)</td>\n",
              "      <td>forget(0.18188486620594188)</td>\n",
              "      <td>new_cbow_300</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Analogy task  ... Correct?\n",
              "0  his is to he as hers is to?  ...    False\n",
              "1  his is to he as hers is to?  ...    False\n",
              "2  his is to he as hers is to?  ...    False\n",
              "3  his is to he as hers is to?  ...    False\n",
              "4  his is to he as hers is to?  ...    False\n",
              "5  his is to he as hers is to?  ...    False\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5h-OWIF9AsKH"
      },
      "source": [
        "## Task 1.4 - Discussion\n",
        "Answer the following question:\n",
        "* Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ssKUZlOBNY9",
        "colab_type": "text"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Given a sentence of n unique words, and a window size of 2L for training. Skipgram will arrage 2L pairs of training samples for each word; therefore, Skipgram has 2nL training samples in total. On the other hand, CBOW uses the context words in the window differently. CBOW combines all 2L context words in the window into one traing sample; thus, CBOW has n training samples in total. The conclusion is that Skipgram has more training samples due to the way it pairs up each word with each of its context words in the window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOa7EXhD-saI"
      },
      "source": [
        "# Question 2 - Peer review (0 pt):\n",
        "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member\n",
        "contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others? You can use the table below to make an overview of how the tasks were divided:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RWdcoJH__MqP"
      },
      "source": [
        "| Student name | Task  |\n",
        "|------|------|\n",
        "|  Huilin Zhu  | task x |\n",
        "| student name 2  | task x|\n",
        "| everyone | task x|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFwRSqYFhKmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Does the order of first 2 words matter?\n",
        "# How to find the predicted word from the predicted word embedding?\n",
        "# How to find the closest word from the predicted word embedding?\n",
        "# What is sim1 and sim2 ..?\n",
        "# to determine closest, we should use mod of the value"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}